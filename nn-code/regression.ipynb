{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a neural network to predict scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from jax.scipy.special import logsumexp\n",
    "from jax import jit, vmap, pmap, grad\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import os\n",
    "from functools import reduce\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize weights and biases in NNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_MLP(layer_widths, parent_key, scale=0.01):\n",
    "\n",
    "    params = []\n",
    "    keys = jax.random.split(parent_key, num=len(layer_widths)-1)\n",
    "\n",
    "    for in_width,out_width,key in zip(layer_widths[:-1],layer_widths[1:],keys):\n",
    "        weight_key, bias_key = jax.random.split(key)\n",
    "        params.append([\n",
    "            scale*jax.random.normal(weight_key, shape=(out_width,in_width)),\n",
    "            scale*jax.random.normal(bias_key,shape=(out_width,))\n",
    "        ])\n",
    "\n",
    "    return params        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return predictions from NNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tanh(params,x):\n",
    "    hidden_layers = params[:-1]\n",
    "    \n",
    "    activation = x\n",
    "    for w,b in hidden_layers:\n",
    "        activation = jax.nn.tanh(jnp.dot(w,activation)+b)\n",
    "\n",
    "    w_last, b_last = params[-1]\n",
    "    result = jnp.dot(w_last,activation) + b_last\n",
    "\n",
    "    return result\n",
    "\n",
    "def predict_relu(params,x):\n",
    "    hidden_layers = params[:-1]\n",
    "    \n",
    "    activation = x\n",
    "    for w,b in hidden_layers:\n",
    "        activation = jax.nn.relu(jnp.dot(w,activation)+b)\n",
    "\n",
    "    w_last, b_last = params[-1]\n",
    "    result = jnp.dot(w_last,activation) + b_last\n",
    "\n",
    "    return result\n",
    "\n",
    "batched_tanh_predict, batched_relu_predict = vmap(predict_tanh,in_axes=(None,0)),vmap(predict_relu,in_axes=(None,0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform functions to put into arrays and to standardize target variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cus_transform(x):\n",
    "    tran = np.ravel(np.array(x,dtype=np.float64))\n",
    "    return tran\n",
    "\n",
    "def cus_collate(batch):\n",
    "    transposed_data = list(zip(*batch))\n",
    "    targets = np.array(transposed_data[1])\n",
    "    features = np.array(transposed_data[0])\n",
    "    return features, targets\n",
    "\n",
    "def cus_target_transform(y,mean=None,std=None):\n",
    "    if not mean:\n",
    "        mean, std = np.mean(y),np.std(y)\n",
    "    return (y-mean)/std, mean, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define custom dataset for simple batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, dir, csv_file, transform=None,target_transform=None,test_transform=None):\n",
    "        self.features = pd.read_csv(os.path.join(dir,csv_file)).to_numpy()\n",
    "        target = csv_file.replace('X','y')\n",
    "        self.target = pd.read_csv(os.path.join(dir,target)).to_numpy()\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.test_transform = test_transform\n",
    "        if self.target_transform and not self.test_transform:\n",
    "            target,mean,std = self.target_transform(self.target)\n",
    "            self.target = target\n",
    "            self.scaler = (mean,std)\n",
    "        if self.test_transform:\n",
    "            target,mean,std = self.target_transform(self.target,mean=self.test_transform[0],std=self.test_transform[1])\n",
    "            self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.features[idx,:]\n",
    "        target = self.target[idx]\n",
    "        if self.transform:\n",
    "            features = self.transform(features)\n",
    "        return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalers_sel = {}\n",
    "i = 0\n",
    "train_dataset = CustomDataset('./splits','sel_X_train_{}.csv'.format(i),target_transform=cus_target_transform)\n",
    "scalers_sel['sel_X_{}'.format(i)] = train_dataset.scaler\n",
    "test_dataset = CustomDataset('./splits','sel_X_test_{}.csv'.format(i),target_transform=cus_target_transform,test_transform=scalers_sel['sel_X_{}'.format(i)])\n",
    "print(type(train_dataset[0][1]),len(train_dataset))#[0][1]))\n",
    "print(type(test_dataset[0][1]),len(test_dataset))#[0][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function and update algorithm - MSE and stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSE_tanh(params, feats, targs):\n",
    "    predictions = batched_tanh_predict(params,feats)\n",
    "\n",
    "    return jnp.mean(jnp.sqrt((predictions - targs)**2))\n",
    "\n",
    "def RMSE_relu(params, feats, targs):\n",
    "    predictions = batched_relu_predict(params,feats)\n",
    "\n",
    "    return jnp.mean(jnp.sqrt((predictions - targs)**2))\n",
    "\n",
    "def MAE_tanh(params, feats, targs):\n",
    "    predictions = batched_tanh_predict(params,feats)\n",
    "\n",
    "    return jnp.mean(abs(predictions - targs))\n",
    "\n",
    "def MAE_relu(params, feats, targs):\n",
    "    predictions = batched_relu_predict(params,feats)\n",
    "\n",
    "    return jnp.mean(abs(predictions - targs))\n",
    "\n",
    "loss_functions = {'RMSE_tanh':RMSE_tanh,'RMSE_relu':RMSE_relu,'MAE_tanh':MAE_tanh,'MAE_relu':MAE_relu}\n",
    "\n",
    "@jit\n",
    "def update_MAE_relu(params, feats, targs, lr=0.05):\n",
    "    grads = grad(MAE_relu)(params, feats, targs)\n",
    "\n",
    "    return jax.tree_map(lambda p,g: p-lr*g,params,grads)\n",
    "\n",
    "@jit\n",
    "def update_MAE_tanh(params, feats, targs, lr=0.05):\n",
    "    grads = grad(MAE_tanh)(params, feats, targs)\n",
    "\n",
    "    return jax.tree_map(lambda p,g: p-lr*g,params,grads)\n",
    "\n",
    "@jit\n",
    "def update_RMSE_relu(params, feats, targs, lr=0.05):\n",
    "    grads = grad(RMSE_relu)(params, feats, targs)\n",
    "\n",
    "    return jax.tree_map(lambda p,g: p-lr*g,params,grads)\n",
    "\n",
    "@jit\n",
    "def update_RMSE_tanh(params, feats, targs, lr=0.05):\n",
    "    grads = grad(RMSE_tanh)(params, feats, targs)\n",
    "\n",
    "    return jax.tree_map(lambda p,g: p-lr*g,params,grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "scalers_sel = {}\n",
    "def run_sel(i,hl,num_epochs,update,predict):\n",
    "    train_dataset = CustomDataset('./splits','sel_X_train_{}.csv'.format(i),target_transform=cus_target_transform)\n",
    "    scalers_sel['sel_X_{}'.format(i)] = train_dataset.scaler\n",
    "    test_dataset = CustomDataset('./splits','sel_X_test_{}.csv'.format(i),target_transform=cus_target_transform,test_transform=scalers_sel['sel_X_{}'.format(i)])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset,batch_size=128,collate_fn=cus_collate,drop_last=True)\n",
    "    first_layer = next(iter(train_loader))[0].shape[1]\n",
    "\n",
    "    key = jax.random.PRNGKey(seed)\n",
    "    MLP_params = init_MLP([first_layer]+hl+[1],key)\n",
    "    t_feats, t_targs = list(zip(*test_dataset))\n",
    "    tmp = update.__name__.lower()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        for feats, targs in train_loader:\n",
    "\n",
    "            MLP_params = update(MLP_params,feats,targs)\n",
    "        \n",
    "        if epoch < 1:\n",
    "            predictions_local = []\n",
    "            if 'rmse' in tmp:\n",
    "                for feats in t_feats:\n",
    "                    predictions_local.append(predict(MLP_params,feats))\n",
    "                predictions_local = np.array(predictions_local)\n",
    "                _loss = np.mean(np.sqrt((predictions_local-t_targs)**2))\n",
    "            else:\n",
    "                for feats in t_feats:\n",
    "                    predictions_local.append(predict(MLP_params,feats))\n",
    "                _loss = np.mean(abs(np.array(predictions_local)-t_targs))\n",
    "            \n",
    "\n",
    "            print('Epoch: {}'.format(epoch+1),'Test Loss: {}'.format(_loss))\n",
    "                \n",
    "        if (epoch+1) % 10 == 0:\n",
    "            predictions_local = []\n",
    "            if 'rmse' in tmp:\n",
    "                for feats in t_feats:\n",
    "                    predictions_local.append(predict(MLP_params,feats))\n",
    "                _loss = np.mean(np.sqrt((np.array(predictions_local)-t_targs)**2))\n",
    "            else:\n",
    "                for feats in t_feats:\n",
    "                    predictions_local.append(predict(MLP_params,feats))\n",
    "                _loss = np.mean(abs(np.array(predictions_local)-t_targs))\n",
    "            \n",
    "\n",
    "            print('Epoch: {}'.format(epoch+1),'Test Loss: {}'.format(_loss))\n",
    "    return predictions_local,t_targs,train_dataset.scaler"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('gen_ml': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "101a92686f9c5d6cbb688e6e828a7aff3a4ab14b010497e53ad928f0aaa169b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
